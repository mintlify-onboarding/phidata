---
title: Conversations
description: **Conversations** are a human-like interface to language models and the starting point for every LLM App. We send the LLM a message and get a model-generated output as a response.
---

Conversations come with built-in **Memory**, **Knowledge**, **Storage** and access to **Tools**. Giving LLMs the ability to have long-term, knowledge-based **Conversations** is the first step in our journey to AGI.

A simple example looks like:

```python conversation.py
from phi.conversation import Conversation

conversation = Conversation()

# -*- Print a response
conversation.print_response('Share a quick healthy breakfast recipe.')

# -*- Get the response as a string
response = conversation.chat('Share a quick healthy breakfast recipe.', stream=False)

# -*- Get the response as a stream
response = ""
for delta in conversation.chat('Share a quick healthy breakfast recipe.'):
    response += delta
```

Under the hood, the message is converted to `System` and `User` prompts that are sent to the LLM. Here we are using the default prompts but they can be easily customized to fit any use case.

Let's walk through come common `Conversation` types

---

## RAG: GPT-4 Powered by PgVector

<Row>
  <Col>

We can improve the quality of LLM reponses by providing additional **Knowledge**, also known as Retrieval Augmented Generation (**RAG**). To enable **RAG**, provide your `Conversation` with a Knowledge Base and Storage.

<Note>

[Run PgVector](/blocks/apps#example-run-pg-vector-on-docker) to try this out.

</Note>

The `add_references_to_prompt` flag will search the **Knowledge Base** for information about the `message` and add it to the **User Prompt** before sending it to the LLM.

### Attributes


  <ResponseField name="llm" type="OpenAIChat">
    The LLM to use for this Conversation.
  </ResponseField>
  <ResponseField name="storage" type="PgConversationStorage">
    Save conversations to a Postgres table. [Read more](/blocks/storage)
  </ResponseField>
  <ResponseField name="knowledge_base" type="PDFKnowledgeBase">
    Convert PDFs into vector embeddings and store them in a Postgres table.
    [Read more](/blocks/knowledge-base#run-pg-vector)
  </ResponseField>
  <ResponseField name="add_references_to_prompt" type="bool">
    Search the knowledge base for documents similar to the input `message` and
    add them to the user prompt.
  </ResponseField>
  <ResponseField name="add_chat_history_to_messages" type="bool">
    Add chat history to the messages sent to the LLM, enabling long-term,
    multi-turn conversations.
  </ResponseField>

<CodeGroup>

```python RAG Conversation
from phi.llm.openai import OpenAIChat
from phi.conversation import Conversation
from phi.storage.conversation.postgres import PgConversationStorage
from phi.knowledge.pdf import PDFUrlKnowledgeBase, PDFKnowledgeBase, PDFReader

rag_conversation = Conversation(
    llm=OpenAIChat(model="gpt-4"),
    # -*- Store conversations in: llm.pdf_conversations
    storage=PgConversationStorage(
        table_name="pdf_conversations",
        db_url=db_url,
    ),
    # -*- Store knowledge base in: llm.pdf_documents
    knowledge_base=PDFKnowledgeBase(
        path="data/pdfs",
        # Table name: llm.pdf_documents
        vector_db=PgVector(
            collection="pdf_documents",
            db_url=db_url,
        ),
    ),
    # -*- Add references to the user prompt (RAG)
    add_references_to_prompt=True,
    # -*- Add chat history to messages
    add_chat_history_to_messages=True,
)
```

</CodeGroup>

---

## Autonomous GPT-4 Powered by PgVector

In Autonomous conversations, we let the LLM decide **if** it needs to access the knowledge base and what search parameters it needs to query the knowledge base.

Behind the scenes, we provide the `Conversation` with a set of `Functions` the LLM can call to achieve tasks.

Set `function_calls=True` to enable function calling and provide default functions to search the **Knowledge Base** and **Memory** when needed.

### Attributes

  <ResponseField name="function_calls" type="bool">
    Makes the conversation `Autonomous` by letting the LLM call functions to
    achieve tasks.
  </ResponseField>
  <ResponseField name="default_functions" type="bool">
    Add default functions to the LLM (default: `True`)
  </ResponseField>
  <ResponseField name="show_function_calls" type="bool">
    Print the signature of called functions.
  </ResponseField>
  <ResponseField name="tools" type="List[Union[Tool, Dict, Callable, Agent]]">
    A list of tools provided to the LLM. [Read
    more](/blocks/conversations#tools)
  </ResponseField>
  <ResponseField name="agents" type="List[Agent]">
    A list of agents provided to the LLM. [Read more](/blocks/agents)
  </ResponseField>

<CodeGroup>

```python Autonomous Conversation
from phi.llm.openai import OpenAIChat
from phi.conversation import Conversation
from phi.storage.conversation.postgres import PgConversationStorage
from phi.knowledge.pdf import PDFUrlKnowledgeBase, PDFKnowledgeBase, PDFReader

auto_conversation = Conversation(
    llm=OpenAIChat(model="gpt-4"),
    # -*- Store conversations in: llm.pdf_conversations
    storage=PgConversationStorage(
        table_name="pdf_conversations",
        db_url=db_url,
    ),
    # -*- Store knowledge base in: llm.pdf_documents
    knowledge_base=PDFKnowledgeBase(
        path="data/pdfs",
        # Table name: llm.pdf_documents
        vector_db=PgVector(
            collection="pdf_documents",
            db_url=db_url,
        ),
    ),
    # -*- Let the LLM call functions to achieve tasks
    function_calls=True,
    # -*- Print the signature of called functions
    show_function_calls=True,
)
```

</CodeGroup>

---

## Prompts

The `Conversation` class merely provides a user-friendly interface to LLMs, meaning behind the scenes it converts the message into `system` and `user` prompts.

These prompts can be updated using the `system_prompt` and `user_prompt_function` attributes.

### Attributes

  <ResponseField name="system_prompt" type="str">
    The `System Prompt` sent to the LLM.
  </ResponseField>
  <ResponseField name="user_prompt_function" type="Callable[..., str]">
    A function which returns the `User Prompt` sent to the LLM.

    Signature:
    ```python
      def custom_user_prompt_function(
          conversation: Conversation,
          message: str,
          references: Optional[str] = None,
          chat_history: Optional[str] = None,
      ) -> str:
          ...
    ```
    This function is provided the `Conversation` object and the input `message` as arguments. It should return the `user_prompt` as a string.

    If `add_references_to_prompt` is True, then `references` are also provided as an argument.

    If `add_chat_history_to_prompt` is True, then `chat_history` is also provided as an argument.

  </ResponseField>

<CodeGroup>

```python Update Prompts for RAG Conversation
rag_conversation = Conversation(
    ...
    system_prompt="Talk to me like a pirate!",
    # The references variable is populated by setting add_references_to_prompt=True
    user_prompt_function=lambda message, references, **kwargs: f"""\
    Use the following information from the knowledge base if it helps.

    START OF KNOWLEDGE BASE
    ---
    {references}
    ---
    END OF KNOWLEDGE BASE

    Your task is to respond to the following message:
    USER: {message}
    ASSISTANT:
    """,
    # -*- Add references to the user prompt
    add_references_to_prompt=True,
    ...
)
```

</CodeGroup>

<CodeGroup>

```python Update Prompts for Autonomous Conversation
auto_conversation = Conversation(
    ...
    system_prompt="Talk to me like a pirate!",
    user_prompt_function=lambda message, **kwargs: f"""\
    Your task is to respond to the following message:
    USER: {message}
    ASSISTANT:
    """,
    ...
)
```

</CodeGroup>

---

## Conversation Class

The `Conversation` class provides an easy to use, human like interface to language models.

### Conversation

<ResponseField name="llm" type="LLM">
  LLM to use for this conversation
</ResponseField>
<ResponseField name="introduction" type="str">
  Add an introduction (from the LLM) to the chat history
</ResponseField>

<ResponseField name="user_name" type="str">
  Name of the user participating in this conversation.
</ResponseField>
<ResponseField name="user_type" type="str">
  Type of the user participating in this conversation.
</ResponseField>

<ResponseField name="id" type="str">
  Unique ID that can be used to continue the conversation across sessions.
</ResponseField>
<ResponseField name="name" type="str">
  Conversation name
</ResponseField>
<ResponseField name="is_active" type="bool">
  True if this conversation is active i.e. not ended
</ResponseField>
<ResponseField name="meta_data" type="Dict[str, Any]">
  Metadata associated with this conversation
</ResponseField>
<ResponseField name="extra_data" type="Dict[str, Any]">
  Extra data associated with this conversation
</ResponseField>
<ResponseField name="created_at" type="datetime">
  The timestamp of when this conversation was created in the database
</ResponseField>
<ResponseField name="updated_at" type="datetime">
  The timestamp of when this conversation was updated in the database
</ResponseField>
<ResponseField name="monitoring" type="bool">
  Monitor conversations on phidata.com
</ResponseField>

<ResponseField name="memory" type="ConversationMemory">
  Conversation Memory
</ResponseField> <ResponseField name="add_chat_history_to_prompt" type="bool">
  Add chat history to the prompt sent to the LLM.
</ResponseField> <ResponseField name="add_chat_history_to_messages" type="bool">
  Add chat history to the messages sent to the LLM.
</ResponseField> <ResponseField name="num_history_messages" type="int">
  Number of previous messages to add to prompt or messages sent to the LLM.
</ResponseField>

<ResponseField name="knowledge_base" type="KnowledgeBase">
  Conversation Knowledge Base
</ResponseField> <ResponseField name="add_references_to_prompt" type="bool">
  Add references from the knowledge base to the prompt sent to the LLM.
</ResponseField>

<ResponseField name="storage" type="ConversationStorage">
  Conversation Storage
</ResponseField> <ResponseField name="create_storage" type="bool">
  Create table if it doesn't exist
</ResponseField>

<ResponseField name="function_calls" type="bool">
  Makes the conversation Autonomous by letting the LLM call functions to achieve
  tasks.
</ResponseField>
<ResponseField name="default_functions" type="bool">
  Add a list of default functions to the LLM
</ResponseField>
<ResponseField name="show_function_calls" type="bool">
  Show function calls in LLM messages.
</ResponseField>
<ResponseField name="functions" type="List[Callable]">
  A list of functions to add to the LLM.
</ResponseField>
<ResponseField name="function_registries" type="List[FunctionRegistry]">
  A list of function registries to add to the LLM.
</ResponseField>

<ResponseField name="system_prompt" type="str">
  Provide the system prompt as a string
</ResponseField>
<ResponseField name="system_prompt_function" type="Callable[..., Optional[str]]">
  Function to build the system prompt.

Signature:

```python
def system_prompt_function(conversation: Conversation) -> str:
    ...
```
</ResponseField>

This function is provided the `Conversation` as an argument and should return the system_prompt as a string.

<ResponseField name="use_default_system_prompt" type="bool">
  If True, the conversation provides a default system prompt
</ResponseField>

<ResponseField name="user_prompt" type="str">
  Provide the user prompt as a string. This will ignore the message provided to the chat function
</ResponseField>
<ResponseField name="user_prompt_function" type="Callable[..., str]">
Function to build the user prompt.

Signature:

```python
def custom_user_prompt_function(
    conversation: Conversation,
    message: str,
    references: Optional[str] = None,
    chat_history: Optional[str] = None,
) -> str:
    ...
```

This function is provided the `Conversation` object and the input `message` as arguments. It should return the `user_prompt` as a string.

If `add_references_to_prompt` is True, then `references` are also provided as an argument.

If `add_chat_history_to_prompt` is True, then `chat_history` is also provided as an argument.

</ResponseField>
<ResponseField name="use_default_user_prompt" type="bool">
If True, the conversation provides a default system user
</ResponseField>

<ResponseField name="references_function" type="Callable[..., Optional[str]]">
Function to build references for the default `user_prompt`. This function, if provided, is called when `add_references_to_prompt` is True

Signature:

```python
def references(conversation: Conversation, query: str) -> Optional[str]:
    ...
```

</ResponseField>
<ResponseField name="chat_history_function" type="Callable[..., Optional[str]]">
Function to build the chat_history for the default `user_prompt`. This function, if provided, is called when `add_chat_history_to_prompt` is True

Signature:

```python
def chat_history(conversation: Conversation, query: str) -> Optional[str]:
    ...
```

</ResponseField>

<ResponseField name="debug_mode" type="bool">
  If True, show debug logs
</ResponseField>