---
title: Build a Streamlit App
description: Use this template to build a **Micro-LLM App** using Streamlit & PgVector, a stack we love for its simplicity. Its simple but not to be underestimated.
---
<Note>

Install <LinkOpenNewTab href="https://docs.docker.com/desktop/install/mac-install/" text="docker desktop" /> to run this app locally.

</Note>

## Setup

Open the `Terminal` and create an `ai` directory with a python virtual environment.

<CodeGroup>

```bash Mac
mkdir ai && cd ai

python3 -m venv aienv
source aienv/bin/activate
```

```bash Windows
mkdir ai; cd ai

python3 -m venv aienv
aienv/scripts/activate
```

</CodeGroup>

Install `phidata`

<CodeGroup>

```bash Mac
pip install -U phidata
```

```bash Windows
pip install -U phidata
```

</CodeGroup>

<Note>

If you encounter errors, update pip using `python -m pip install --upgrade pip`

</Note>

## Create your codebase

Create your codebase using the `streamlit-app` template pre-configured with <LinkOpenNewTab href="https://fastapi.tiangolo.com/" text="Streamlit" /> and <LinkOpenNewTab href="https://github.com/pgvector/pgvector" text="PgVector" />. We'll use this codebase as the starting point for our LLM product.

<CodeGroup>

```bash Mac
phi ws create -t streamlit-app -n streamlit-app
```

```bash Windows
phi ws create -t streamlit-app -n streamlit-app
```

</CodeGroup>

This will create a folder named `streamlit-app` with the following structure:

```bash streamlit-app
streamlit-app               # root directory for your streamlit-app
├── app                     # directory for Streamlit apps
├── db                      # directory for database components
├── llm                     # directory for LLM components
    ├── conversations       # LLM conversations
    ├── knowledge_base.py   # LLM knowledge base
    └── storage.py          # LLM storage
├── Dockerfile              # Dockerfile for the application
├── pyproject.toml          # python project definition
├── requirements.txt        # python dependencies generated by pyproject.toml
├── scripts                 # directory for helper scripts
├── tests                   # directory for unit tests
├── utils                   # directory for shared utilities
└── workspace               # phidata workspace directory
    ├── dev_resources.py    # dev resources running locally
    ├── prd_resources.py    # production resources running on AWS
    ├── secrets             # directory for storing secrets
    └── settings.py         # phidata workspace settings
```

### Optional: Set OpenAI Api Key

To use your own `OPENAI_API_KEY`, set the `OPENAI_API_KEY` environment variable. You can get one <LinkOpenNewTab href="https://platform.openai.com/account/api-keys" text="from OpenAI here" />. Otherwise `phi` provides ready to use access to OpenAI models.

<CodeGroup>

```bash Mac
export OPENAI_API_KEY=sk-***
```

```bash Windows
setx OPENAI_API_KEY sk-***
```

</CodeGroup>

## Local Streamlit App

<LinkOpenNewTab href="https://streamlit.io/" text="Streamlit" /> allows us to build
micro front-ends for our LLM App and is extremely useful for building basic applications
in pure python. Start your workspace using:

<CodeGroup>

```bash Terminal
phi ws up
```

```bash With Options
phi ws up --env dev --infra docker
```

```bash Shorthand
phi ws up dev:docker
```

</CodeGroup>

**Press Enter** to confirm and give a few minutes for the image to download (only the first time). Verify container status and view logs on the docker dashboard.

### Chat with PDFs

- Open <LinkOpenNewTab href="http://localhost:8501" text="localhost:8501" /> to view streamlit apps that you can customize and make your own.
- Click on **Chat with PDFs** in the sidebar
- Enter a username and wait for the knowledge base to load.
- Choose between `RAG` or `Autonomous` mode.
- Ask "How do I make chicken curry?"
- Upload PDFs and ask questions

<ImageContainer
  src="/images/rag-llm-app-chat-with-pdf.png"
  alt="Streamlit App - chat with pdf"
/>

## Add your data

The `pdf_knowledge_base` in the `llm/knowledge_base.py` file provides the context used by the LLM. The `PDFKnowledgeBase` reads local PDFs and the `PDFUrlKnowledgeBase` reads PDFs from URLs.

<Note>

**Make this app your own** by creating a `data/pdfs` folder with your PDFs. Then click on the `Update Knowledge Base` button to load the knowledge base.

</Note>

```python llm/knowledge_base.py
url_pdf_knowledge_base = PDFUrlKnowledgeBase(
  ...
)

local_pdf_knowledge_base = PDFKnowledgeBase(
    path="data/pdfs",
    ...
)

pdf_knowledge_base = CombinedKnowledgeBase(
    sources=[
        url_pdf_knowledge_base,
        local_pdf_knowledge_base,
    ],
    num_documents=2, # 2 references are added to the prompt.
    ...
)
```

## How this App works

The streamlit apps are defined in the `app` folder and the `Conversations` powering these apps are defined in the `llm/conversations` folder.

**Checkout the `llm/conversations/pdf_rag.py` file for the `RAG` conversation:**

- The `add_references_to_prompt` flag will populate the `references` argument of the `user_prompt_function`
- The `user_prompt_function` builds the **Prompt** sent to the LLM. The prompt is injected with additional information from the knowledge base using the `references` variable.

```python llm/conversations/pdf_rag.py
...
def get_pdf_rag_conversation(
    user_name: Optional[str] = None,
    conversation_id: Optional[str] = None,
    debug_mode: bool = False,
) -> Conversation:
    """Get a RAG conversation with the PDF knowledge base"""

    return Conversation(
        ...
        user_prompt_function=lambda message, references, **kwargs: f"""\
        Use the following information from the knowledge base if it helps.
        <knowledge_base>
        {references}
        </knowledge_base>

        Respond to the following message:
        USER: {message}
        ASSISTANT:
        """,
        # This setting populates the "references" variable to the user prompt function
        add_references_to_prompt=True,
        # This setting adds the last 8 messages to the API call
        add_chat_history_to_messages=True,
    )
```

**Checkout the `llm/conversations/pdf_auto.py` file for the `Autonomous` conversation:**

- The `function_calls` flag gives the LLM the ability to call functions.
- The `show_function_calls` prints which functions the LLM is running.

```python llm/conversations/pdf_rag.py
...
def get_pdf_auto_conversation(
    user_name: Optional[str] = None,
    conversation_id: Optional[str] = None,
    debug_mode: bool = False,
) -> Conversation:
    """Get an autonomous conversation with the PDF knowledge base"""

    return Conversation(
        ...
        function_calls=True,
        show_function_calls=True,
        ...
    )
```

## Delete local resources

Play around and stop the workspace using:

<CodeGroup>

```bash Terminal
phi ws down
```

```bash With Options
phi ws down --env dev --infra docker
```

```bash Shorthand
phi ws down dev:docker
```

</CodeGroup>

---

## Run on AWS

Now let's run the **Streamlit App** in production on AWS.

### AWS Authentication

To run on AWS, you need **one** of the following:

1. The `~/.aws/credentials` file with your AWS credentials
2. `AWS_ACCESS_KEY_ID` + `AWS_SECRET_ACCESS_KEY` environment variables

<Note>

To create the credentials file, install the <LinkOpenNewTab href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html" text="aws cli" /> and run `aws configure`

</Note>

### Add AWS Region and Subnets

Add 2 <LinkOpenNewTab href="https://us-east-1.console.aws.amazon.com/vpc/home?#subnets:" text="Subnets" /> to the `workspace/settings.py` file, these are required for the ECS services.

```python workspace/settings.py
ws_settings = WorkspaceSettings(
    ...
    # -*- AWS settings
    # Add your Subnet IDs here
    subnet_ids=["subnet-xyz", "subnet-xyz"],
    ...
)
```

<Note>

Please check that the subnets belong to the selected `aws_region`

</Note>

## Update Secrets

Update the streamlit app password in the `workspace/secrets/prd_app_secrets.yml` file:

```python workspace/secrets/prd_app_secrets.yml
APP_PASSWORD: "admin"
# OPENAI_API_KEY: "sk-***"
```

Update the RDS database password in the `workspace/secrets/prd_db_secrets.yml` file:

```python workspace/secrets/prd_db_secrets.yml
# Secrets used by prd RDS database
MASTER_USERNAME: llm
MASTER_USER_PASSWORD: "llm9999!!"
```

## Create AWS resources

Create AWS resources for the LLM Api using:

<CodeGroup>

```bash Terminal
phi ws up --env prd --infra aws
```

```bash Shorthand
phi ws up prd:aws
```

</CodeGroup>

This will create:

1. [ECS Cluster](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/clusters.html) for the application.
2. [ECS Task Definitions](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html) and [Services](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html) that run the application on the ECS cluster.
3. [LoadBalancer](https://aws.amazon.com/elasticloadbalancing/) to route traffic to the application.
4. [Security Groups](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) that control incoming and outgoing traffic.
5. [Secrets](https://aws.amazon.com/secrets-manager/) for managing application and database secrets.
6. [RDS Database](https://aws.amazon.com/rds/) for Knowledge Base and Storage.

**Press Enter** to confirm and grab a cup of coffee while the resources spin up.

- The RDS database takes about 10 minutes to activate.
- These resources are defined in the `workspace/prd_resources.py` file.
- Use the [ECS console](https://us-east-1.console.aws.amazon.com/ecs/v2/clusters) to view services and logs.
- Use the [RDS console](https://us-east-1.console.aws.amazon.com/rds/home?#databases:) to view the database instance.

## Production Streamlit App

**Open the LoadBalancer DNS** provided when creating the Streamlit App

### Chat with PDF

- Enter the APP_PASSWORD from the `prd_app_secrets.yml` file (default: `admin`)
- Click on **Chat with PDFs** in the sidebar
- Enter a username and wait for the knowledge base to load.
- Choose between `RAG` or `Autonomous` mode.
- Ask questions about different recipes.

<ImageContainer
  src="/images/llm-app-chat-with-pdf-prd.png"
  alt="llm-app-chat-with-pdf-prd"
/>

## Update Production

To update the production application, follow [this guide](/day-2/production-app) to

1. Create a new image
2. Update the ECS Task Definition and Services.

## Delete AWS resources

Play around and then delete AWS resources using:

<CodeGroup>

```bash Terminal
phi ws down --env prd --infra aws
```

```bash Shorthand
phi ws down prd:aws
```

</CodeGroup>

or delete individual resource groups using:

<CodeGroup>

```bash App
phi ws down --env prd --infra aws --group app
```

```bash Database
phi ws down --env prd --infra aws --group db
```

</CodeGroup>

## Next

Congratulations on building a prodution-ready Micro-LLM App. Next Steps:

- Modify this App to your use case.
- Create a [git repository for this workspace](/day-2/git-repo).
- Read how to [update the dev application](/day-2/dev-app).
- Read how to [update the production application](/day-2/production-app).
- Read how to [format and validate your code](/day-2/format-and-validate).
- Read how to [add python libraries](/day-2/python-libraries).
- Read how to [add database tables](/day-2/database-tables)
- Read how to [add a custom domain and HTTPS](/day-2/domain-https).
- Read how to [implement CI/CD](/day-2/ci-cd)
- Learn about other [day-2 operations](/day-2/day-2-operations).
- Chat with us on <LinkOpenNewTab href="https://discord.gg/4MtYHHrgA8" text="discord" />.

---
