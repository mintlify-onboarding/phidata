---
title: Build a RAG LLM App
---

## What is RAG?

Retrieval Augmented Generation means providing the LLM with additional information along with the question to improve its responses. Here's how it works:

- Without RAG, a user asks the LLM a question and the LLM responds with information from its training data.
- With RAG, a user asks the LLM a question and we "extend the question" with relevant information from a knowledge base. The LLM then responds in a context-aware manner.
- **Using RAG is like taking an open-book test.**

Let's build a RAG LLM App powered by GPT-4. We'll use PgVector for Knowledge Base and Storage and serve the app using Streamlit and FastApi.

By the end of this guide you'll be ready to build your own RAG LLM product.

<Note>

Install <LinkOpenNewTab href="https://docs.docker.com/desktop/install/mac-install/" text="docker desktop" /> to run this app locally.

</Note>

## Setup

Open the `Terminal` and create an `ai` directory with a python virtual environment.

<CodeGroup>

```bash Mac
mkdir ai && cd ai

python3 -m venv aienv
source aienv/bin/activate
```

```bash Windows
mkdir ai; cd ai

python3 -m venv aienv
aienv/scripts/activate
```

</CodeGroup>

Install `phidata`

<CodeGroup>

```bash Mac
pip install -U phidata
```

```bash Windows
pip install -U phidata
```

</CodeGroup>

<Note>

If you encounter errors, update pip using `python -m pip install --upgrade pip`

</Note>

## Create your codebase

Create your codebase using the `llm-app` template pre-configured with <LinkOpenNewTab href="https://fastapi.tiangolo.com/" text="FastApi" />, <LinkOpenNewTab href="https://streamlit.io/" text="Streamlit" /> and <LinkOpenNewTab href="https://github.com/pgvector/pgvector" text="PgVector" />. We'll use this codebase as the starting point for our RAG LLM App.

<CodeGroup>

```bash Mac
phi ws create -t llm-app -n llm-app
```

```bash Windows
phi ws create -t llm-app -n llm-app
```

</CodeGroup>

<ImageContainer src="/gifs/create_llm_app.gif" alt="Creat LLM App" />

This will create a folder named `llm-app` with the following structure:

```bash llm-app
llm-app                     # root directory for your RAG llm-app
├── api                     # directory for FastApi routes
├── app                     # directory for Streamlit apps
├── db                      # directory for database components
├── llm                     # directory for LLM components
    ├── conversations       # LLM conversations
    ├── knowledge_base.py   # LLM knowledge base
    └── storage.py          # LLM storage
├── notebooks               # directory for Jupyter notebooks
├── Dockerfile              # Dockerfile for the application
├── pyproject.toml          # python project definition
├── requirements.txt        # python dependencies managed by pyproject.toml
├── scripts                 # directory for helper scripts
├── tests                   # directory for unit tests
├── utils                   # directory for shared utilities
└── workspace               # phidata workspace directory
    ├── dev_resources.py    # dev resources running locally
    ├── prd_resources.py    # production resources running on AWS
    ├── jupyter             # jupyter notebook resources
    ├── secrets             # directory for storing secrets
    └── settings.py         # phidata workspace settings
```

### Optional: Set OpenAI Api Key

To use your own `OPENAI_API_KEY`, set the `OPENAI_API_KEY` environment variable. You can get one <LinkOpenNewTab href="https://platform.openai.com/account/api-keys" text="from OpenAI here" />. Otherwise `phi` provides ready to use access to OpenAI models.

<CodeGroup>

```bash Mac
export OPENAI_API_KEY=sk-***
```

```bash Windows
setx OPENAI_API_KEY sk-***
```

</CodeGroup>

## Serve your App using Streamlit

<LinkOpenNewTab href="https://streamlit.io/" text="Streamlit" /> allows us to build
micro front-ends for our LLM App and is extremely useful for building basic applications
in pure python. Start the `app` group using:

<CodeGroup>

```bash Terminal
phi ws up --group app
```

```bash Shorthand
phi ws up dev:docker:app
```

</CodeGroup>

<ImageContainer src="/gifs/phi_ws_up_app.gif" alt="Run LLM App" />

**Press Enter** to confirm and give a few minutes for the image to download (only the first time). Verify container status and view logs on the docker dashboard.

### Chat with PDFs

- Open <LinkOpenNewTab href="http://localhost:8501" text="localhost:8501" /> to view streamlit apps that you can customize and make your own.
- Click on **Chat with PDFs** in the sidebar
- Enter a username and wait for the knowledge base to load.
- Choose the `RAG` Conversation type.
- Ask "How do I make chicken curry?"
- Upload PDFs and ask questions

<ImageContainer
  src="/images/rag-llm-app-chat-with-pdf.png"
  alt="Streamlit App - chat with pdf"
/>

- Check the docker logs to see the "context" passed along with the question.

<ImageContainer
  src="/images/rag-llm-app-docker-logs.png"
  alt="RAG logs - chat with pdf"
/>

## Add your data

The `pdf_knowledge_base` in the `llm/knowledge_base.py` file provides the references used for RAG. The `PDFKnowledgeBase` reads local PDFs and the `PDFUrlKnowledgeBase` reads PDFs from URLs.

<Note>

**Make this app your own** by creating a `data/pdfs` folder with your PDFs. Then click on the `Update Knowledge Base` button to load the knowledge base.

</Note>

```python llm/knowledge_base.py
url_pdf_knowledge_base = PDFUrlKnowledgeBase(
  ...
)

local_pdf_knowledge_base = PDFKnowledgeBase(
    path="data/pdfs",
    ...
)

pdf_knowledge_base = CombinedKnowledgeBase(
    sources=[
        url_pdf_knowledge_base,
        local_pdf_knowledge_base,
    ],
    num_documents=2, # 2 references are added to the prompt.
    ...
)
```

## How this App works

The streamlit apps are defined in the `app` folder and the `Conversations` powering these apps are defined in the `llm/conversations` folder. Checkout the `llm/conversations/pdf_rag.py` file:

- The `add_references_to_prompt` flag will populate the `references` argument of the `user_prompt_function`
- The `user_prompt_function` builds the **Prompt** sent to the LLM. The prompt is injected with additional information from the knowledge base using the `references` variable.

```python llm/conversations/pdf_rag.py
...
def get_pdf_rag_conversation(
    user_name: Optional[str] = None,
    conversation_id: Optional[str] = None,
    debug_mode: bool = False,
) -> Conversation:
    """Get a RAG conversation with the PDF knowledge base"""

    return Conversation(
        ...
        user_prompt_function=lambda message, references, **kwargs: f"""\
        Use the following information from the knowledge base if it helps.
        <knowledge_base>
        {references}
        </knowledge_base>

        Respond to the following message:
        USER: {message}
        ASSISTANT:
        """,
        # This setting populates the "references" argument of the user prompt function
        add_references_to_prompt=True,
        # This setting adds the last 8 messages to the API call
        add_chat_history_to_messages=True,
    )
```

## Serve your App using FastApi

Streamlit is great for building micro front-ends but any production application will be built using a front-end framework like <LinkOpenNewTab href="https://nextjs.org" text="next.js" /> backed by a RestApi built using <LinkOpenNewTab href="https://fastapi.tiangolo.com/" text="FastApi" />.

Your LLM App comes ready-to-use with FastApi endpoints, start the `api` group using:

<CodeGroup>

```bash Terminal
phi ws up --group api
```

```bash Shorthand
phi ws up dev:docker:api
```

</CodeGroup>

**Press Enter** to confirm and give a few minutes for the image to download.

### View API Endpoints

- Open <LinkOpenNewTab href="http://localhost:8000/docs" text="localhost:8000/docs" /> to view the API Endpoints.
- Load the knowledge base using `/v1/pdf/conversation/load-knowledge-base`
- Test the `v1/pdf/conversation/chat` endpoint with `{"message": "How do I make chicken curry?"}`

<ImageContainer
  src="/images/rag-llm-app-fastapi-local.png"
  alt="rag-llm-app-fastapi-local"
/>

## Build your LLM Product

Your LLM Api comes with common endpoints you can use to build your LLM product. These routes are developed in collaboration with real LLM Apps and provide a great starting point to build on.

For example:

- Call the `/conversation/create` endpoint to create a new conversation for a user.

```json
{
  "user_name": "my-app-user-1",
  "conversation_type": "RAG"
}
```

- The response contains a `conversation_id` that can be used to build a chat interface by calling the `/conversation/chat` endpoint. Message us on <LinkOpenNewTab href="https://discord.gg/4MtYHHrgA8" text="discord" /> if you need help.

```json
{
  "message": "how do I make pasta",
  "stream": true,
  "conversation_id": "fc67f202-c2cb-4c3d-a640-704ab83d9460",
  "conversation_type": "RAG"
}
```

The FastApi routes are defined the `api/routes` folder and can be customized to your use case.

## Optional: Run Jupyterlab

A jupyter notebook is a must have for AI development and your `llm-app` comes with a notebook pre-installed with the required dependencies. Enable it by updating the `workspace/settings.py` file:

```python workspace/settings.py
...
ws_settings = WorkspaceSettings(
    ...
    # Uncomment the following line
    dev_jupyter_enabled=True,
...
```

Start `jupyter` using:

<CodeGroup>

```bash Terminal
phi ws up --group jupyter
```

```bash Shorthand
phi ws up dev:docker:jupyter
```

</CodeGroup>

**Press Enter** to confirm and give a few minutes for the image to download (only the first time). Verify container status and view logs on the docker dashboard.

### View Jupyterlab UI

- Open <LinkOpenNewTab href="http://localhost:8888" text="localhost:8888" /> to view the Jupyterlab UI. Password: **admin**
- Play around with cookbooks in the `notebooks` folder.

<ImageContainer
  src="/images/rag-llm-app-jupyter-local.png"
  alt="rag-llm-app-jupyter-local"
/>

## Delete local resources

Play around and stop the workspace using:

<CodeGroup>

```bash Terminal
phi ws down
```

```bash With Options
phi ws down --env dev --infra docker
```

```bash Shorthand
phi ws down dev:docker
```

</CodeGroup>

or stop individual Apps using:

<CodeGroup>

```bash App
phi ws down --group app
```

```bash Api
phi ws down --group api
```

```bash PgVector
phi ws down --group db
```

```bash Jupyter
phi ws down --group jupyter
```

</CodeGroup>

---

## Run on AWS

Now let's run the **LLM App** in production on AWS.

## AWS Authentication

To run on AWS, you need **one** of the following:

1. The `~/.aws/credentials` file with your AWS credentials
2. `AWS_ACCESS_KEY_ID` + `AWS_SECRET_ACCESS_KEY` environment variables

<Note>

To create the credentials file, install the <LinkOpenNewTab href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html" text="aws cli" /> and run `aws configure`

</Note>

### Add AWS Region and Subnets

Add 2 <LinkOpenNewTab href="https://us-east-1.console.aws.amazon.com/vpc/home?#subnets:" text="Subnets" /> to the `workspace/settings.py` file, these are required for the ECS services.

```python workspace/settings.py
ws_settings = WorkspaceSettings(
    ...
    # -*- AWS settings
    # Add your Subnet IDs here
    subnet_ids=["subnet-xyz", "subnet-xyz"],
    ...
)
```

<Note>

Please check that the subnets belong to the selected `aws_region`

</Note>

## Update Secrets

Update the streamlit app password in the `workspace/secrets/prd_app_secrets.yml` file:

```python workspace/secrets/prd_app_secrets.yml
APP_PASSWORD: "admin"
# OPENAI_API_KEY: "sk-***"
```

Update the RDS database password in the `workspace/secrets/prd_db_secrets.yml` file:

```python workspace/secrets/prd_db_secrets.yml
# Secrets used by prd RDS database
MASTER_USERNAME: llm
MASTER_USER_PASSWORD: "llm9999!!"
```

## Create AWS resources

Create AWS resources for the LLM App using:

<CodeGroup>

```bash Terminal
phi ws up --env prd --infra aws
```

```bash Shorthand
phi ws up prd:aws
```

</CodeGroup>

This will create:

1. [ECS Cluster](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/clusters.html) for the application.
2. [ECS Task Definitions](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task_definitions.html) and [Services](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/ecs_services.html) that run the application on the ECS cluster.
3. [LoadBalancer](https://aws.amazon.com/elasticloadbalancing/) to route traffic to the application.
4. [Security Groups](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html) that control incoming and outgoing traffic.
5. [Secrets](https://aws.amazon.com/secrets-manager/) for managing application and database secrets.
6. [RDS Database](https://aws.amazon.com/rds/) for Knowledge Base and Storage.

**Press Enter** to confirm and grab a cup of coffee while the resources spin up.

- The RDS database takes about 10 minutes to activate.
- These resources are defined in the `workspace/prd_resources.py` file.
- Use the [ECS console](https://us-east-1.console.aws.amazon.com/ecs/v2/clusters) to view services and logs.
- Use the [RDS console](https://us-east-1.console.aws.amazon.com/rds/home?#databases:) to view the database instance.

<ImageContainer
  src="/gifs/create_llm_app_aws.gif"
  alt="create_llm_app_aws.gif"
/>

## Production Streamlit App

**Open the LoadBalancer DNS** provided when creating the Streamlit App

### Chat with PDF

- Enter the `APP_PASSWORD` from the `prd_app_secrets.yml` file (default: `admin`)
- Click on **Chat with PDFs** in the sidebar
- Enter a username and wait for the knowledge base to load.
- Choose the `RAG` Conversation type.
- Ask "How do I make chicken curry?"

<ImageContainer
  src="/images/rag-llm-app-chat-with-pdf-prd.png"
  alt="rag-llm-app-chat-with-pdf-prd"
/>

## Production FastApi

- **Open the LoadBalancer DNS** + the `/docs` endpoint to view the API Endpoints.
- Load the knowledge base using `/v1/pdf/conversation/load-knowledge-base`
- Test the `v1/pdf/conversation/chat` endpoint with `{"message": "How do I make chicken curry?"}`
- Update and integrate with your front-end or product.

<ImageContainer
  src="/images/rag-llm-app-fastapi-prd.png"
  alt="rag-llm-app-fastapi-prd"
/>

## Update Production

To update the production application, follow [this guide](/day-2/production-app) to

1. Create a new image
2. Update the ECS Task Definition and Services.

## Delete AWS resources

Play around and then delete AWS resources using:

<CodeGroup>

```bash Terminal
phi ws down --env prd --infra aws
```

```bash Shorthand
phi ws down prd:aws
```

</CodeGroup>

or delete individual resource groups using:

<CodeGroup>

```bash App
phi ws down --env prd --infra aws --group app
```

```bash Database
phi ws down --env prd --infra aws --group db
```

</CodeGroup>

## Next

Congratulations on building a production-ready RAG LLM App with access to a knowledge base and storage. Next Steps:

- Make this App your own by adding your data.
- Create a [git repository for this workspace](/day-2/git-repo).
- Read how to [update the dev application](/day-2/dev-app).
- Read how to [update the production application](/day-2/production-app).
- Read how to [format and validate your code](/day-2/format-and-validate).
- Read how to [add python libraries](/day-2/python-libraries).
- Read how to [add database tables](/day-2/database-tables)
- Read how to [add a custom domain and HTTPS](/day-2/domain-https).
- Read how to [implement CI/CD](/day-2/ci-cd)
- Learn about other [day-2 operations](/day-2/day-2-operations).
- Chat with us on <LinkOpenNewTab href="https://discord.gg/4MtYHHrgA8" text="discord" />.
